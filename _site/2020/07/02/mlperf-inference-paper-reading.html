<h1 id="mlperf-inference-paper-reading">MLPerf Inference Paper Reading</h1>

<h2 id="1-mlperf-supporting-organizations">1. MLPerf Supporting Organizations</h2>

<ol>
  <li>28 Companies
    <ul>
      <li>Alibaba</li>
      <li>AMD</li>
      <li>ARM</li>
      <li>Baidu</li>
      <li>Google</li>
      <li>Huawei</li>
      <li>Intel</li>
      <li>NVDIA</li>
      <li>…</li>
    </ul>
  </li>
  <li>500+ discuusion group members</li>
  <li>7 Research Instituions
    <ul>
      <li>Harvard University</li>
      <li>Stanford University</li>
      <li>University of California, berkerly</li>
      <li>…</li>
    </ul>
  </li>
</ol>

<h2 id="2-deep-learning-benchmark-challeges">2. Deep Learning Benchmark Challeges</h2>

<ol>
  <li>Diversity of models
    <ul>
      <li>Model name fail to uniquely describe a model</li>
      <li>Community support, open source</li>
    </ul>
  </li>
  <li>Deployment-Senario Diversity</li>
  <li>Inference-System Diversity</li>
  <li>Something addtions
    <ul>
      <li>require industry-wide input</li>
      <li>using latency other than MACs</li>
      <li>accuracy is crucial</li>
    </ul>
  </li>
</ol>

<h2 id="3-contributions">3. Contributions</h2>

<ol>
  <li>Models Chosen</li>
  <li>Identify Deep Learning realistic senarios</li>
  <li>Defines Metrics</li>
  <li>Defines allows and prohibited techniques, benchmarking rules</li>
  <li>Extendable</li>
</ol>

<h2 id="4-mlperf-inferenc-benchmark-design">4. MLPerf Inferenc Benchmark Design</h2>

<ol>
  <li><strong>Models and datasets chosen</strong>
    <ul>
      <li>lightweigh and heavyweight</li>
      <li>Vision and Language</li>
    </ul>
  </li>
  <li><strong>Defines quanlity targets</strong>
    <ul>
      <li>per-model quanlity targets</li>
      <li>within 1% FP32 reference model’s accuracy</li>
      <li>no-retrainning</li>
      <li>mobilenet within 2% FP32 reference accuary, 22.0 mAP</li>
    </ul>
  </li>
  <li><strong>Realistic End-User Scenarios</strong>
    <ul>
      <li>Single-stream</li>
      <li>Multistream</li>
      <li>Server</li>
      <li>Offline</li>
    </ul>
  </li>
  <li><strong>Statistically Confident Tail-Latency Bounds</strong>
    <ul>
      <li>Sample size defined by tail-latency percentage, confidence and margin</li>
      <li>run at least 60 seconds for dynamic voltage and freqency scaling(DVFS) consideration</li>
    </ul>
  </li>
</ol>

<h2 id="5-comparasion">5. Comparasion</h2>

<ul>
  <li><strong>AI Benchmark</strong>
Only foncus on Android smartphones and only measure latency</li>
  <li><strong>EEMBC MLMark</strong>
Only measure performance and accuracy of embedded inference devices, fixed best batch size</li>
  <li><strong>Fathom</strong>
Focus on throughput rather than accuracy</li>
  <li><strong>AI Matrix</strong>
Alibaba’s Standard, focus on basic operation like convolution and matrix multiplication</li>
  <li><strong>DeepBench</strong>
fail to addess the complexity of full models</li>
  <li><strong>TBD(Training Benchmarks for DNNs)</strong>
Focus on ML training, focus on GPU only</li>
  <li><strong>DAWNBench</strong>
Inspire MLPerf</li>
</ul>

<h2 id="6-mlperf-inference-summary">6. <a href="https://mlperf.org/inference-overview">MLPerf Inference Summary</a></h2>
